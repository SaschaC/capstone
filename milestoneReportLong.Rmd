---
title: Coursera Data Science - Capstone Project
subtitle: Trigram Model Word Prediction
author: "Sascha C."
date: "3 december 2016"
output: 
  html_document:
    toc: true
    toc_float: true
    number_sections: true
abstract: This report investigates unigram, bigram, and trigram frequencies in a Twitter, a Blog, and a News corpus. Each of the three corpora is provided by Swiftkey (download [here](https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip)). A preview of a word prediction algorithm that is based on trigram language models derived from the corpora is given.
---
# Tokenization
In this section, samples from each of the three corpora are tokenized into unigrams, bigrams and trigrams. First, the three corpora are read in, and samples are taken from each corpus. 
```{r, warning=FALSE, cache=TRUE,warning=F,message=F}
require(data.table)
require(ggplot2)
library(dplyr)
library(tidytext)
library(tidyr)
library(tokenizers)
#reading in corpora
n<-100000
twitter<-file("./corpora/en_US/en_US.twitter.txt")
blogs<-file("./corpora/en_US/en_US.blogs.txt")
news<-file("./corpora/en_US/en_US.news.txt")
twitter_f=readLines(twitter,encoding = "UTF-8", n=n)
blogs_f=readLines(blogs,encoding = "UTF-8", n=n)
news_f=readLines(news,encoding = "UTF-8", n=n)
close(twitter)
close(blogs)
close(news)
#sample function 
get_sample<-function(data,sampleSize){
  sample(data,floor(sampleSize*length(data)),replace=F)
}
#taking samples
sampleSize=0.03
twitter_sample<-get_sample(twitter_f,sampleSize)%>%paste(collapse=" ")
twitter_sample<-iconv(twitter_sample, from="UTF-8", to="ASCII", sub="")
blogs_sample<-get_sample(blogs_f,sampleSize)%>%paste(collapse=" ")
blogs_sample<-iconv(blogs_sample, from="UTF-8", to="ASCII", sub="")
news_sample<-get_sample(news_f,sampleSize)%>%paste(collapse=" ")
news_sample<-iconv(news_sample, from="UTF-8", to="ASCII", sub="")
```
Table 1 gives a breakdown of corpora and sample sizes:
```{r table1, echo=FALSE, message=FALSE, warning=FALSE, results='asis'}
require(pander)
table<-data.frame(Twitter=c(length(twitter),length(twitter_sample),
                            (length(twitter_sample)/length(twitter))*100), 
                  Blogs=c(length(blogs),length(blogs_sample),
                          (length(blogs_sample)/length(blogs))*100),
                  News=c(length(news),length(news_sample),
                         (length(news_sample)/length(news))*100)
)
rownames(table)=c("Corpus size (n)","Sample size (n)", "Sample size (%)")
panderOptions('table.split.table', Inf)
set.caption("Table 1. Size of corpora and corresponding samples; n = number of lines.")
pander(table, style = 'rmarkdown')
rm(blogs,news,twitter)
```

Next, the samples are tokenized into unigrams, bigrams, and trigrams. This is done with the ```tokenize_ngrams()``` function from the [tokenizers R package](https://cran.r-project.org/web/packages/tokenizers/index.html) (the tokenizer currently doesn't remove digits, and it handles special characters poorly). The tokens are saved in data tables from the [data.table R package](https://cran.r-project.org/web/packages/data.table/index.html).
```{r, warning=FALSE, cache=TRUE}
#collapsing sample vectors into single element vectors
twitter_sample<-twitter_sample%>%paste(collapse=" ")
blogs_sample<-blogs_sample%>%paste(collapse=" ")
news_sample<-news_sample%>%paste(collapse=" ")
#tokenize function
token_df<- function(text,corpus_name,ngrams){
  t=tokenize_ngrams(text, lowercase = TRUE, n = ngrams,
                    stopwords = character(), ngram_delim = " ", simplify = F)  
  name_vector=rep(corpus_name,length(t[[1]]))
  d=data.table(term=t[[1]], corpus=name_vector)
}
#Creating data tables for unigrams, bigrams, trigrams
start.time <- Sys.time()
set.seed(1122)
unigramTokens<-rbind(token_df(twitter_sample,"twitter",1L),token_df(blogs_sample,"blogs",1L),token_df(news_sample,"news",1L))
bigramTokens<-rbind(token_df(twitter_sample,"twitter",2L),token_df(blogs_sample,"blogs",2L),token_df(news_sample,"news",2L))
trigramTokens<-rbind(token_df(twitter_sample,"twitter",3L),token_df(blogs_sample,"blogs",3L),token_df(news_sample,"news",3L))
end.time <- Sys.time()
time.taken <- end.time - start.time
message("Tokenization took ",time.taken," seconds.")
unigramTokens[1,];bigramTokens[1,];trigramTokens[1,]
```

#  Exploratory analyses
## Unigram frequency
What are the most frequent unigrams in the corpora samples, and how are unigram frequencies distributed across the samples?
In order to answer this question, I calculated the **term frequency** of each unigram in the three corpora samples by dividing the token frequency by the total sum of tokens in the corresponding samples:  
```{r, warning=FALSE, cache=TRUE}
# Calculating frequencies
dtFrequencies<-unigramTokens[,.(.N),by=.(term,corpus)]
dtFrequencies[,total:=(sum(N)),by=corpus]
dtFrequencies[,termFrequency:=(N/total),by=corpus]
```
What are the three most frequent words in each sample, and how are unigram frequencies ditributed?
```{r, warning=FALSE, cache=TRUE}
# Getting the three most frequent unigrams for each sample
dtFrequencies%>%
  group_by(corpus) %>% 
  arrange(desc(termFrequency))%>% 
  top_n(3) %>% ungroup
# Plot histogram of frequencies
plotDtFrequencies<-ggplot(dtFrequencies, aes(termFrequency,fill=corpus)) +
  geom_histogram(show.legend = FALSE)+
  xlim(NA, 0.0025) +
  facet_wrap(~corpus, ncol = 1, scales = "free_y")
plotDtFrequencies
```

## Inverse document frequency
What are the 10 most important unigrams in each sample?  
To answer this, I calculated the **[inverse document frequencies](https://en.wikipedia.org/wiki/Tf-idf) (idf)** of unigrams using the ```bind_tf_idf()``` function from the [tidytext R package](http://tidytextmining.com/tfidf.html). The **idf** of a unigram is intended to reflect how important a term is to a document in a collection or corpus: 
```{r, warning=FALSE, cache=TRUE}
dtFrequencies<- dtFrequencies %>%
  bind_tf_idf(term, corpus, N)%>%
  group_by(corpus) %>% 
  arrange(desc(tf_idf))%>% 
  top_n(10) %>% ungroup %>%
  mutate(term = factor(term, levels =unique(term)))
plotTfIdfFrequencies<-ggplot(dtFrequencies, aes(term, tf_idf, fill = corpus)) +
  geom_bar(alpha = 0.8, stat = "identity", show.legend = FALSE) +
  labs(title = "Highest idf words in Corpora",
       x = NULL, y = "idf") +
  facet_wrap(~corpus, ncol = 3,scales="free")+
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
plotTfIdfFrequencies
```

## Token coverage
What proportion of all tokens do the the 100 most frequent unigrams cover? First, I computed overall unigram token frequencies and sorted them by frequency:
```{r, warning=FALSE, cache=TRUE}
unigramFrequencies<-unigramTokens[,.(.N),by=term]
unigramFrequencies<-unigramFrequencies[order(-rank(N))]
```
Then, I created a vector with the cumulative unigram frequency sum of the frequency ordered unigrams:
```{r, warning=FALSE, cache=TRUE}
i<-2; cumSum<-unigramFrequencies[1,N]
for(i in 2:nrow(unigramFrequencies)){cumSum=c(cumSum,cumSum[i-1]+unigramFrequencies[i,N])}
unigramFrequencies[,cumSum:=cumSum/sum(N)]
# Plot cumulative sum of frequency ordered unigrams
plotFrequencies<-ggplot(unigramFrequencies[1:100,], aes(as.integer(rownames(unigramFrequencies[1:100])), cumSum))+
  geom_point()+
  theme(axis.text.x = element_text(angle = 90, hjust = 1))+
  scale_x_continuous(breaks=seq(0,nrow(unigramFrequencies),10))+xlab("Frequency rank")
plotFrequencies
```

As can be seen in the graph, the three most frequent unigrams account for around 10% of all tokens, and the 100 most frequent unigrams for around 45%. How many of the most frequent unigrams do you need for a 90% coverage?
```{r, warning=FALSE, cache=TRUE}
message("The most frequent ", min(which(unigramFrequencies[,cumSum] > 0.9))," unigrams cover 90% of all unigram tokens.")
```

# Preview: trigram model word prediction
This section develops a word prediction algorithm based on trigram models. The predicted word of the algorithm is the word that combines with the two preceding words to the most probable trigram. The probability of trigrams is based on maximum likelhoods. The algorithm uses [stupid backoff](https://lagunita.stanford.edu/c4x/Engineering/CS-224N/asset/slp4.pdf) (p.20) as a smoothing method, currently without backoff weights. 

## Maximum likelihood (ML) computation
First, I calculated the maximum likelhoods of unigrams, bigrams, and trigrams. For the calculation of bigram and trigram MLs, I used fast [subsetting based on keys](https://cran.r-project.org/web/packages/data.table/vignettes/datatable-keys-fast-subset.html):
```{r, warning=FALSE, cache=TRUE}
start.time <- Sys.time()
unigramFrequencies<-unigramTokens[,.(.N),by=term]
setkey(unigramFrequencies,term)
unigramFrequencies[,ml:=N/sum(N)]%>%arrange(desc(N))
bigramFrequencies<-bigramTokens[,.(.N),by=term]%>%
  separate(term, c("word1", "word2"), sep = " ")
setkey(bigramFrequencies,word1,word2)
bigramFrequencies[,ml:=N/unigramFrequencies[.(word1)]$N]%>%arrange(desc(N))
trigramFrequencies<-trigramTokens[,.(.N),by=term]%>%
  separate(term, c("word1", "word2","word3"), sep = " ")
setkey(trigramFrequencies,word1,word2,word3)
trigramFrequencies[,ml:=N/bigramFrequencies[.(trigramFrequencies[,word1],trigramFrequencies[,word2])]$N]%>%arrange(desc(N))
end.time <- Sys.time()
time.taken <- end.time - start.time
message("ML computation took ",time.taken," seconds.")
```

## Word prediction algorithm
The word prediction algorithm implements [stupid backoff](https://lagunita.stanford.edu/c4x/Engineering/CS-224N/asset/slp4.pdf) (p.20) and is based on fast [subsetting based on keys](https://cran.r-project.org/web/packages/data.table/vignettes/datatable-keys-fast-subset.html). It uses the key columns as defined in [3.1](## Maximum likelihood (ML)). The algorithm proceeds in four steps:
```{r, warning=FALSE, cache=TRUE}
start.time <- Sys.time()
input<-c("coffee","and")
#1.
matchTrigram<-trigramFrequencies[.(input[1],input[2],unigramFrequencies[,term])]%>%arrange(desc(ml))
#2.
matchBigram<-bigramFrequencies[.(input[2],matchTrigram[is.na(N),word3])]%>%arrange(desc(ml))
#3.
matchUnigram<-unigramFrequencies[.(matchBigram[is.na(N),word2])]%>%arrange(desc(ml))
#4.
matchTrigram[1:5,];matchBigram[1:5,];matchUnigram[1:5,]
end.time <- Sys.time()
time.taken <- end.time - start.time
message("Run time was ",time.taken," seconds.")
```
## Still to do
1. Computing weights for backoff algorithm.
2. Possibly implement other smoothing techniques.
3. Improve string normalization and tokenizer (the tokenizer seems to have issues with formatting)
4. Implement Shiny app
